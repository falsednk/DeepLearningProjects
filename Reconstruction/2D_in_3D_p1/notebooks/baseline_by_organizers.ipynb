{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0714157d",
   "metadata": {
    "papermill": {
     "duration": 0.010836,
     "end_time": "2024-05-21T11:16:36.630902",
     "exception": false,
     "start_time": "2024-05-21T11:16:36.620066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\"> üì∏ Image Matching Challenge - üìä Understanding the baseline</center>\n",
    "<p><center style=\"color:#949494; font-family: consolas; font-size: 20px;\">Reconstruct 3D scenes from 2D images over six different domains</center></p>\n",
    "\n",
    "***\n",
    "\n",
    "In this notebook I explain the baseline solution provided by the organizers in [this notebook](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n",
    "\n",
    "I have made the code a bit easier to read, adding comments and type annotations to make it easier for you to understand what is going on.\n",
    "\n",
    "Hope you enjoy ‚ù§Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c334b9",
   "metadata": {
    "papermill": {
     "duration": 0.009265,
     "end_time": "2024-05-21T11:16:36.649362",
     "exception": false,
     "start_time": "2024-05-21T11:16:36.640097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Structure from Motion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0759721f",
   "metadata": {
    "papermill": {
     "duration": 0.009557,
     "end_time": "2024-05-21T11:16:36.668551",
     "exception": false,
     "start_time": "2024-05-21T11:16:36.658994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Structure from Motion (SfM) is the name given to the procedure of **reconstructing a 3D scene and simultaneously obtaining the camera poses of a camera w.r.t. the given scene**. This means that, as the name suggests, we are creating the entire rigid structure from a set of images with different view points (or equivalently a camera in motion).\n",
    "\n",
    "In this competition, the important aspect of SfM we are interested in is *obtaining the camera poses* of where each image was taken, described by a rotation matrix and translation vector from the origin. These are the objects that will be scored in our submission!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50722a3",
   "metadata": {
    "papermill": {
     "duration": 0.009002,
     "end_time": "2024-05-21T11:16:36.686839",
     "exception": false,
     "start_time": "2024-05-21T11:16:36.677837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center><img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse1.mm.bing.net%2Fth%3Fid%3DOIP.ENP48SmZHwG3r3O0lUVcWAHaFf%26pid%3DApi&f=1&ipt=550bf79efa85e7af870dd2d0a16793af7f3f83a36c14a1f4a648659a384b5a98&ipo=images\" alt=\"Structure from motion structure: multiple cameras pointing toward an object in different positions and rotations that we need to find.\"></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e8791c",
   "metadata": {
    "papermill": {
     "duration": 0.009773,
     "end_time": "2024-05-21T11:16:36.707503",
     "exception": false,
     "start_time": "2024-05-21T11:16:36.697730",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Baseline solution steps\n",
    "In order to be able to estimate the camera poses, the solution provided by the organizers consists in the following steps:\n",
    "\n",
    "* [1. Find pairs of images that are similar](#1)\n",
    "* [2. Compute image keypoints](#2)\n",
    "* [3. Match keypoints between images](#3)\n",
    "* [4. Outlier detection with RANSAC](#4)\n",
    "* [5. Sparse reconstruction](#5)\n",
    "\n",
    "Let's understand how these steps are carried out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5458578",
   "metadata": {
    "papermill": {
     "duration": 0.009274,
     "end_time": "2024-05-21T11:16:36.726368",
     "exception": false,
     "start_time": "2024-05-21T11:16:36.717094",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Installing & importing relevant packages and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59869e4e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-21T11:16:36.747251Z",
     "iopub.status.busy": "2024-05-21T11:16:36.746895Z",
     "iopub.status.idle": "2024-05-21T11:16:49.807672Z",
     "shell.execute_reply": "2024-05-21T11:16:49.806562Z"
    },
    "papermill": {
     "duration": 13.074397,
     "end_time": "2024-05-21T11:16:49.810107",
     "exception": false,
     "start_time": "2024-05-21T11:16:36.735710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/extractors-points-v2/depth-save.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/extractors-points-v2/disk_lightglue.pth /root/.cache/torch/hub/checkpoints/disk_lightglue_v0-1_arxiv-pth\n",
    "!cp /kaggle/input/extractors-points-v2/superpoint_v1.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/* /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/* /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth\n",
    "!cp /kaggle/input/extractors-points-v2/superpoint_lightglue.pth /root/.cache/torch/hub/checkpoints/superpoint_lightglue_v0-1_arxiv-pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60adbb34",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-05-21T11:16:49.831679Z",
     "iopub.status.busy": "2024-05-21T11:16:49.831354Z",
     "iopub.status.idle": "2024-05-21T11:17:07.467944Z",
     "shell.execute_reply": "2024-05-21T11:17:07.467044Z"
    },
    "papermill": {
     "duration": 17.650053,
     "end_time": "2024-05-21T11:17:07.470300",
     "exception": false,
     "start_time": "2024-05-21T11:16:49.820247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from time import time, sleep\n",
    "from fastprogress import progress_bar\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from typing import Any\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from torch import Tensor as T\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "import torch\n",
    "from lightglue import match_pair\n",
    "from lightglue import LightGlue, ALIKED, SuperPoint\n",
    "from lightglue.utils import load_image, rbd\n",
    "\n",
    "import pycolmap\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/input/colmap-db-import\")\n",
    "\n",
    "from database import *\n",
    "from h5_to_db import *\n",
    "\n",
    "def arr_to_str(a):\n",
    "    \"\"\"Returns ;-separated string representing the input\"\"\"\n",
    "    return \";\".join([str(x) for x in a.reshape(-1)])\n",
    "\n",
    "def load_torch_image(file_name: Path | str, device=torch.device(\"cpu\")):\n",
    "    \"\"\"Loads an image and adds batch dimension\"\"\"\n",
    "    img = K.io.load_image(file_name, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "\n",
    "device = K.utils.get_cuda_device_if_available(0)\n",
    "print(device)\n",
    "\n",
    "DEBUG = len([p for p in Path(\"/kaggle/input/image-matching-challenge-2024/test/\").iterdir() if p.is_dir()]) == 2\n",
    "print(\"DEBUG:\", DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149d58e",
   "metadata": {
    "papermill": {
     "duration": 0.010164,
     "end_time": "2024-05-21T11:17:07.491060",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.480896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"1\"></a>\n",
    "# Finding image pairs\n",
    "\n",
    "To find pairs of similar images, we use [DINOv2](https://arxiv.org/pdf/2304.07193.pdf) to obtain normalized image embeddings.\n",
    "\n",
    "<center><img src=\"https://www.labellerr.com/blog/content/images/2023/05/Dino-v2-20230419.jpg\" alt=\"DINOv2 example\"></center> \n",
    "Then, we calculate the distances between all the embeddings, and only keep those below a given distance threshold. For images with less than a set minimum number of pairs, the closest ones are kept instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ed3442",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:17:07.513144Z",
     "iopub.status.busy": "2024-05-21T11:17:07.512618Z",
     "iopub.status.idle": "2024-05-21T11:17:07.521340Z",
     "shell.execute_reply": "2024-05-21T11:17:07.520611Z"
    },
    "papermill": {
     "duration": 0.021918,
     "end_time": "2024-05-21T11:17:07.523292",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.501374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embed_images(\n",
    "    paths: list[Path],\n",
    "    model_name: str,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> T:\n",
    "    \"\"\"Computes image embeddings.\n",
    "    \n",
    "    Returns a tensor of shape [len(filenames), output_dim]\n",
    "    \"\"\"\n",
    "    processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).eval().to(device)\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for i, path in tqdm(enumerate(paths), desc=\"Global descriptors\"):\n",
    "        image = load_torch_image(path)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=image, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            outputs = model(**inputs) \n",
    "            embedding = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=-1, p=2)\n",
    "            \n",
    "        embeddings.append(embedding.detach().cpu())\n",
    "    return torch.cat(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75d4d7ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:17:07.545177Z",
     "iopub.status.busy": "2024-05-21T11:17:07.544916Z",
     "iopub.status.idle": "2024-05-21T11:17:07.555091Z",
     "shell.execute_reply": "2024-05-21T11:17:07.554222Z"
    },
    "papermill": {
     "duration": 0.023264,
     "end_time": "2024-05-21T11:17:07.557071",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.533807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pairs_exhaustive(lst: list[Any]) -> list[tuple[int, int]]:\n",
    "    \"\"\"Obtains all possible index pairs of a list\"\"\"\n",
    "    return list(itertools.combinations(range(len(lst)), 2))            \n",
    "    \n",
    "def get_image_pairs(\n",
    "    paths: list[Path],\n",
    "    model_name: str,\n",
    "    similarity_threshold: float = 0.6,\n",
    "    tolerance: int = 1000,\n",
    "    min_matches: int = 20,\n",
    "    exhaustive_if_less: int = 20,\n",
    "    p: float = 2.0,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> list[tuple[int, int]]:\n",
    "    \"\"\"Obtains pairs of similar images\"\"\"\n",
    "    if len(paths) <= exhaustive_if_less:\n",
    "        return get_pairs_exhaustive(paths)\n",
    "    \n",
    "    matches = []\n",
    "\n",
    "    embeddings = embed_images(paths, model_name)\n",
    "    distances = torch.cdist(embeddings, embeddings, p=p)\n",
    "\n",
    "    mask = distances <= similarity_threshold\n",
    "    image_indices = np.arange(len(paths))\n",
    "    \n",
    "    for current_image_index in range(len(paths)):\n",
    "        mask_row = mask[current_image_index]\n",
    "        indices_to_match = image_indices[mask_row]\n",
    "        \n",
    "        if len(indices_to_match) < min_matches:\n",
    "            indices_to_match = np.argsort(distances[current_image_index])[:min_matches]\n",
    "            \n",
    "        for other_image_index in indices_to_match:\n",
    "            if other_image_index == current_image_index:\n",
    "                continue\n",
    "\n",
    "            if distances[current_image_index, other_image_index] < tolerance:\n",
    "                matches.append(tuple(sorted((current_image_index, other_image_index.item()))))\n",
    "                \n",
    "    return sorted(list(set(matches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d592ac2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:17:07.578506Z",
     "iopub.status.busy": "2024-05-21T11:17:07.578195Z",
     "iopub.status.idle": "2024-05-21T11:17:07.582716Z",
     "shell.execute_reply": "2024-05-21T11:17:07.581870Z"
    },
    "papermill": {
     "duration": 0.017549,
     "end_time": "2024-05-21T11:17:07.584796",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.567247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    images_list = list(Path(\"/kaggle/input/image-matching-challenge-2024/test/church/images/\").glob(\"*.png\"))[:10]\n",
    "    index_pairs = get_image_pairs(images_list, '/kaggle/input/dinov2/pytorch/base/1')\n",
    "    print(index_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd77bb5",
   "metadata": {
    "papermill": {
     "duration": 0.009959,
     "end_time": "2024-05-21T11:17:07.605177",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.595218",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"2\"></a>\n",
    "# Computing keypoints\n",
    "\n",
    "In order to be able to know the position of each camera, we must be able to relate images to each other. For this, we extract relevant keypoints and compare pairs of image keypoints against each other. There are many ways to extract relevant keypoints, the most traditional one being [SIFT](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform). However, newer and improved methods exist now, one of which is [ALIKED](https://arxiv.org/abs/2304.03608), the keypoint extraction method used in the solution.\n",
    "\n",
    "<center><img src=\"https://www.catalyzex.com/_next/image?url=https%3A%2F%2Fd3i71xaburhd42.cloudfront.net%2Faf9fc17471b4c38211c3d9f5058c9c1f59501eea%2F3-Figure1-1.png&w=640&q=75\" alt=\"ALIKED architecture\"></center> \n",
    "\n",
    "\n",
    "Let's take a closer look at the keypoints that ALIKED extracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aa00e71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:17:07.626492Z",
     "iopub.status.busy": "2024-05-21T11:17:07.626221Z",
     "iopub.status.idle": "2024-05-21T11:17:07.634106Z",
     "shell.execute_reply": "2024-05-21T11:17:07.633263Z"
    },
    "papermill": {
     "duration": 0.020681,
     "end_time": "2024-05-21T11:17:07.636119",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.615438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    dtype = torch.float32 \n",
    "    extractor = ALIKED(\n",
    "            max_num_keypoints=6000, \n",
    "            detection_threshold=0.01, \n",
    "            resize=2560\n",
    "        ).eval().to(device, dtype)\n",
    "\n",
    "    path = images_list[0]\n",
    "    image = get_image_tensor(image_path_or_array=path, resize=True, resize_shape=2560, scale = True, resize_longest_edge=True, grayscale=True).to(dtype).to(device)\n",
    "    features = extractor.extract(image)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 20))\n",
    "    ax[0].imshow(image[0, ...].permute(1,2,0).cpu())\n",
    "    ax[1].imshow(image[0, ...].permute(1,2,0).cpu())\n",
    "    ax[1].scatter(features[\"keypoints\"][0, :, 0].cpu(), features[\"keypoints\"][0, :, 1].cpu(), s=0.5, c=\"red\")\n",
    "\n",
    "    del extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce06795e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:17:07.657232Z",
     "iopub.status.busy": "2024-05-21T11:17:07.656962Z",
     "iopub.status.idle": "2024-05-21T11:17:07.665245Z",
     "shell.execute_reply": "2024-05-21T11:17:07.664441Z"
    },
    "papermill": {
     "duration": 0.020854,
     "end_time": "2024-05-21T11:17:07.667101",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.646247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_keypoints (\n",
    "    paths: list[Path],\n",
    "    feature_dir: Path,\n",
    "    num_features: int = 4096,\n",
    "    resize_to: int = 1024,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> None:\n",
    "    \"\"\"Detects the keypoints in a list of images with ALIKED\n",
    "    \n",
    "    Stores them in feature_dir/keypoints.h5 and feature_dir/descriptors.h5\n",
    "    to be used later with LightGlue\n",
    "    \"\"\"\n",
    "    dtype = torch.float32 \n",
    "    \n",
    "    extractor = ALIKED(\n",
    "        max_num_keypoints=num_features, \n",
    "        detection_threshold=0.01, \n",
    "        resize=resize_to\n",
    "    ).eval().to(device, dtype)\n",
    "    \n",
    "    feature_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with h5py.File(feature_dir / \"keypoints.h5\", mode=\"w\") as f_keypoints, \\\n",
    "         h5py.File(feature_dir / \"descriptors.h5\", mode=\"w\") as f_descriptors:\n",
    "        \n",
    "        for path in tqdm(paths, desc=\"Computing keypoints\"):\n",
    "            key = path.name\n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                image = load_torch_image(path, device=device).to(dtype)\n",
    "                features = extractor.extract(image)\n",
    "                f_keypoints[key] = features[\"keypoints\"].squeeze().detach().cpu().numpy()\n",
    "                f_descriptors[key] = features[\"descriptors\"].squeeze().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2d285d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:17:07.687778Z",
     "iopub.status.busy": "2024-05-21T11:17:07.687523Z",
     "iopub.status.idle": "2024-05-21T11:17:07.691282Z",
     "shell.execute_reply": "2024-05-21T11:17:07.690502Z"
    },
    "papermill": {
     "duration": 0.016236,
     "end_time": "2024-05-21T11:17:07.693110",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.676874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    feature_dir = Path(\"./sample_test_features\")\n",
    "    detect_keypoints(images_list, feature_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c68f3e",
   "metadata": {
    "papermill": {
     "duration": 0.009646,
     "end_time": "2024-05-21T11:17:07.712560",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.702914",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"3\"></a>\n",
    "# Match and compute keypoint distances\n",
    "\n",
    "Now that we have the relevant image pairs and keypoints, we can go ahead and compare the keypoints of the images in a pair to find a good relationship between them. This is done with [LightGlue](https://arxiv.org/abs/2306.13643), which matches the keypoints and their descriptors between two images.\n",
    "\n",
    "<center><img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fraw.githubusercontent.com%2Fcvg%2Flightglue%2Fmaster%2Fassets%2Feasy_hard.jpg&f=1&nofb=1&ipt=60962b56b05d3e8f95a064ab2a6010e5a6cbd5f1d10379d90e660b2561a3bae9&ipo=images\" alt=\"LightGlue example\"></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28d05dba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:17:07.733279Z",
     "iopub.status.busy": "2024-05-21T11:17:07.733021Z",
     "iopub.status.idle": "2024-05-21T11:17:07.741766Z",
     "shell.execute_reply": "2024-05-21T11:17:07.740955Z"
    },
    "papermill": {
     "duration": 0.021258,
     "end_time": "2024-05-21T11:17:07.743693",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.722435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    matcher_params = {\n",
    "        \"width_confidence\": -1,\n",
    "        \"depth_confidence\": -1,\n",
    "        \"mp\": True if 'cuda' in str(device) else False,\n",
    "    }\n",
    "    matcher = KF.LightGlueMatcher(\"aliked\", matcher_params).eval().to(device) \n",
    "\n",
    "    with h5py.File(feature_dir / \"keypoints.h5\", mode=\"r\") as f_keypoints, \\\n",
    "         h5py.File(feature_dir / \"descriptors.h5\", mode=\"r\") as f_descriptors:\n",
    "            idx1, idx2 = index_pairs[0]\n",
    "            key1, key2 = images_list[idx1].name, images_list[idx2].name\n",
    "\n",
    "            keypoints1 = torch.from_numpy(f_keypoints[key1][...]).to(device)\n",
    "            keypoints2 = torch.from_numpy(f_keypoints[key2][...]).to(device)\n",
    "            print(\"Keypoints:\", keypoints1.shape, keypoints2.shape)\n",
    "            descriptors1 = torch.from_numpy(f_descriptors[key1][...]).to(device)\n",
    "            descriptors2 = torch.from_numpy(f_descriptors[key2][...]).to(device)\n",
    "            print(\"Descriptors:\", descriptors1.shape, descriptors2.shape)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                distances, indices = matcher(\n",
    "                    descriptors1, \n",
    "                    descriptors2, \n",
    "                    KF.laf_from_center_scale_ori(keypoints1[None]),\n",
    "                    KF.laf_from_center_scale_ori(keypoints2[None]),\n",
    "                )\n",
    "    print(distances, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b8eb793",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:17:07.764786Z",
     "iopub.status.busy": "2024-05-21T11:17:07.764529Z",
     "iopub.status.idle": "2024-05-21T11:17:07.776326Z",
     "shell.execute_reply": "2024-05-21T11:17:07.775543Z"
    },
    "papermill": {
     "duration": 0.024878,
     "end_time": "2024-05-21T11:17:07.778238",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.753360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def keypoint_distances(\n",
    "    paths: list[Path],\n",
    "    index_pairs: list[tuple[int, int]],\n",
    "    feature_dir: Path,\n",
    "    min_matches: int = 15,\n",
    "    verbose: bool = True,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> None:\n",
    "    \"\"\"Computes distances between keypoints of images.\n",
    "    \n",
    "    Stores output at feature_dir/matches.h5\n",
    "    \"\"\"\n",
    "    \n",
    "    matcher_params = {\n",
    "        \"width_confidence\": -1,\n",
    "        \"depth_confidence\": -1,\n",
    "        \"mp\": True if 'cuda' in str(device) else False,\n",
    "    }\n",
    "    matcher = KF.LightGlueMatcher(\"aliked\", matcher_params).eval().to(device)\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    with h5py.File(feature_dir / \"keypoints.h5\", mode=\"r\") as f_keypoints, \\\n",
    "         h5py.File(feature_dir / \"descriptors.h5\", mode=\"r\") as f_descriptors, \\\n",
    "         h5py.File(feature_dir / \"matches.h5\", mode=\"w\") as f_matches:\n",
    "        \n",
    "            for idx1, idx2 in tqdm(index_pairs, desc=\"Computing keypoing distances\"):\n",
    "                key1, key2 = paths[idx1].name, paths[idx2].name\n",
    "                \n",
    "\n",
    "                keypoints1 = torch.from_numpy(f_keypoints[key1][...]).to(device)\n",
    "                keypoints2 = torch.from_numpy(f_keypoints[key2][...]).to(device)\n",
    "                descriptors1 = torch.from_numpy(f_descriptors[key1][...]).to(device)\n",
    "                descriptors2 = torch.from_numpy(f_descriptors[key2][...]).to(device)\n",
    "\n",
    "                with torch.inference_mode():\n",
    "                    distances, indices = matcher(\n",
    "                        descriptors1, \n",
    "                        descriptors2, \n",
    "                        KF.laf_from_center_scale_ori(keypoints1[None]),\n",
    "                        KF.laf_from_center_scale_ori(keypoints2[None]),\n",
    "                    )\n",
    "\n",
    "                # We have matches to consider\n",
    "                n_matches = len(indices)\n",
    "                if n_matches:\n",
    "                    if verbose:\n",
    "                        print(f\"{key1}-{key2}: {n_matches} matches\")\n",
    "                    # Store the matches in the group of one image\n",
    "                    if n_matches >= min_matches:\n",
    "                        group  = f_matches.require_group(key1)\n",
    "                        group.create_dataset(key2, data=indices.detach().cpu().numpy().reshape(-1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a94c3627",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:17:07.798993Z",
     "iopub.status.busy": "2024-05-21T11:17:07.798687Z",
     "iopub.status.idle": "2024-05-21T11:17:07.803223Z",
     "shell.execute_reply": "2024-05-21T11:17:07.802442Z"
    },
    "papermill": {
     "duration": 0.01693,
     "end_time": "2024-05-21T11:17:07.805030",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.788100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    keypoint_distances(images_list, index_pairs, feature_dir, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d1836",
   "metadata": {
    "papermill": {
     "duration": 0.0099,
     "end_time": "2024-05-21T11:17:07.824759",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.814859",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"4\"></a>\n",
    "# RANSAC\n",
    "Up to now, we have matched keypoints and their descriptors extracted from pairs of images. This is described by a [fundamental matrix](https://en.wikipedia.org/wiki/Fundamental_matrix_(computer_vision)) denoted as $F$. In epipolar geometry, with homogeneous image coordinates, $x$ and $x‚Ä≤$, of corresponding points in a stereo image pair, $Fx$ describes a line (an epipolar line) on which the corresponding point $x‚Ä≤$ on the other image must lie. That means, for all pairs of corresponding points, $x'Fx = 0$ holds. This is known as epipolar constraint or correspondance condition (or Longuet-Higgins equation), and is solved via the [eight-point algorithm](https://en.wikipedia.org/wiki/Eight-point_algorithm).\n",
    "\n",
    "<center><img src=\"https://cmsc426.github.io/assets/sfm/epipole1.png\" alt=\"Fundamental matrix\"></center>\n",
    "\n",
    "Since the keypoint correspondences are computed using feature descriptors, the data is bound to be noisy and (in general) contains several outliers. Thus, to remove these outliers, we use a [RANSAC](https://en.wikipedia.org/wiki/Random_sample_consensus) algorithm to find the best possible fundamental matrix. So, out of all possibilities, the $F$ matrix with maximum number of inliers is chosen.\n",
    "\n",
    "<center><img src=\"https://cmsc426.github.io/assets/sfm/ransac.png\" alt=\"RANSAC\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "729718c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:17:07.847743Z",
     "iopub.status.busy": "2024-05-21T11:17:07.847470Z",
     "iopub.status.idle": "2024-05-21T11:17:07.853414Z",
     "shell.execute_reply": "2024-05-21T11:17:07.852470Z"
    },
    "papermill": {
     "duration": 0.01964,
     "end_time": "2024-05-21T11:17:07.855490",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.835850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_into_colmap(\n",
    "    path: Path,\n",
    "    feature_dir: Path,\n",
    "    database_path: str = \"colmap.db\",\n",
    ") -> None:\n",
    "    \"\"\"Adds keypoints into colmap\"\"\"\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, path, \"\", \"simple-pinhole\", single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46000a6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:17:07.877690Z",
     "iopub.status.busy": "2024-05-21T11:17:07.877442Z",
     "iopub.status.idle": "2024-05-21T11:17:07.881959Z",
     "shell.execute_reply": "2024-05-21T11:17:07.881068Z"
    },
    "papermill": {
     "duration": 0.017729,
     "end_time": "2024-05-21T11:17:07.883897",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.866168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    database_path = \"colmap.db\"\n",
    "    images_dir = images_list[0].parent\n",
    "    import_into_colmap(\n",
    "        images_dir, \n",
    "        feature_dir, \n",
    "        database_path,\n",
    "    )\n",
    "\n",
    "    # This does RANSAC\n",
    "    pycolmap.match_exhaustive(database_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b5dd7a",
   "metadata": {
    "papermill": {
     "duration": 0.010162,
     "end_time": "2024-05-21T11:17:07.904653",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.894491",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"5\"></a>\n",
    "# Sparse Reconstruction\n",
    "\n",
    "Now we have similar image pairs, with matched keypoint descriptors, without outliers! All that is left is to construct the scene and obtain the camera positions. We do this with pycolmap, which offers an incremental reconstruction algorithm that starts from two pairs of images and continually adds more and more images to the scene, resulting in a reconstructed scene with camera information. We can then use the camera rotation and translation as our submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e94d6049",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-05-21T11:17:07.926772Z",
     "iopub.status.busy": "2024-05-21T11:17:07.926492Z",
     "iopub.status.idle": "2024-05-21T11:17:07.931474Z",
     "shell.execute_reply": "2024-05-21T11:17:07.930542Z"
    },
    "papermill": {
     "duration": 0.018334,
     "end_time": "2024-05-21T11:17:07.933340",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.915006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    mapper_options = pycolmap.IncrementalPipelineOptions()\n",
    "    mapper_options.min_model_size = 3\n",
    "    mapper_options.max_num_models = 2\n",
    "\n",
    "    maps = pycolmap.incremental_mapping(\n",
    "        database_path=database_path, \n",
    "        image_path=images_dir,\n",
    "        output_path=Path.cwd() / \"incremental_pipeline_outputs\", \n",
    "        options=mapper_options,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9137f0b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:17:07.955191Z",
     "iopub.status.busy": "2024-05-21T11:17:07.954921Z",
     "iopub.status.idle": "2024-05-21T11:17:07.959759Z",
     "shell.execute_reply": "2024-05-21T11:17:07.958888Z"
    },
    "papermill": {
     "duration": 0.017641,
     "end_time": "2024-05-21T11:17:07.961671",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.944030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(maps[0].summary())\n",
    "    for k, im in maps[0].images.items():\n",
    "        print(\"Rotation\", im.cam_from_world.rotation.matrix(), \"Translation:\", im.cam_from_world.translation, sep=\"\\n\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab22b7f",
   "metadata": {
    "papermill": {
     "duration": 0.010042,
     "end_time": "2024-05-21T11:17:07.981902",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.971860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Running everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6719264c",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-21T11:17:08.002935Z",
     "iopub.status.busy": "2024-05-21T11:17:08.002433Z",
     "iopub.status.idle": "2024-05-21T11:17:08.009719Z",
     "shell.execute_reply": "2024-05-21T11:17:08.008883Z"
    },
    "papermill": {
     "duration": 0.019901,
     "end_time": "2024-05-21T11:17:08.011629",
     "exception": false,
     "start_time": "2024-05-21T11:17:07.991728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_sample_submission(\n",
    "    base_path: Path,\n",
    ") -> dict[dict[str, list[Path]]]:\n",
    "    \"\"\"Construct a dict describing the test data as \n",
    "    \n",
    "    {\"dataset\": {\"scene\": [<image paths>]}}\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    with open(base_path / \"sample_submission.csv\", \"r\") as f:\n",
    "        for i, l in enumerate(f):\n",
    "            # Skip header\n",
    "            if i == 0:\n",
    "                print(\"header:\", l)\n",
    "\n",
    "            if l and i > 0:\n",
    "                image_path, dataset, scene, _, _ = l.strip().split(',')\n",
    "                if dataset not in data_dict:\n",
    "                    data_dict[dataset] = {}\n",
    "                if scene not in data_dict[dataset]:\n",
    "                    data_dict[dataset][scene] = []\n",
    "                data_dict[dataset][scene].append(Path(base_path / image_path))\n",
    "\n",
    "    for dataset in data_dict:\n",
    "        for scene in data_dict[dataset]:\n",
    "            print(f\"{dataset} / {scene} -> {len(data_dict[dataset][scene])} images\")\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9ea8c5f",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-21T11:17:08.032481Z",
     "iopub.status.busy": "2024-05-21T11:17:08.032215Z",
     "iopub.status.idle": "2024-05-21T11:17:08.040746Z",
     "shell.execute_reply": "2024-05-21T11:17:08.039913Z"
    },
    "papermill": {
     "duration": 0.021232,
     "end_time": "2024-05-21T11:17:08.042797",
     "exception": false,
     "start_time": "2024-05-21T11:17:08.021565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_submission(\n",
    "    results: dict,\n",
    "    data_dict: dict[dict[str, list[Path]]],\n",
    "    base_path: Path,\n",
    ") -> None:\n",
    "    \"\"\"Prepares a submission file.\"\"\"\n",
    "    \n",
    "    with open(\"submission.csv\", \"w\") as f:\n",
    "        f.write(\"image_path,dataset,scene,rotation_matrix,translation_vector\\n\")\n",
    "        \n",
    "        for dataset in data_dict:\n",
    "            # Only write results for datasets with images that have results \n",
    "            if dataset in results:\n",
    "                res = results[dataset]\n",
    "            else:\n",
    "                res = {}\n",
    "            \n",
    "            # Same for scenes\n",
    "            for scene in data_dict[dataset]:\n",
    "                if scene in res:\n",
    "                    scene_res = res[scene]\n",
    "                else:\n",
    "                    scene_res = {\"R\":{}, \"t\":{}}\n",
    "                    \n",
    "                # Write the row with rotation and translation matrices\n",
    "                for image in data_dict[dataset][scene]:\n",
    "                    if image in scene_res:\n",
    "                        print(image)\n",
    "                        R = scene_res[image][\"R\"].reshape(-1)\n",
    "                        T = scene_res[image][\"t\"].reshape(-1)\n",
    "                    else:\n",
    "                        R = np.eye(3).reshape(-1)\n",
    "                        T = np.zeros((3))\n",
    "                    image_path = str(image.relative_to(base_path))\n",
    "                    f.write(f\"{image_path},{dataset},{scene},{arr_to_str(R)},{arr_to_str(T)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92527d2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:17:08.063516Z",
     "iopub.status.busy": "2024-05-21T11:17:08.063230Z",
     "iopub.status.idle": "2024-05-21T11:17:08.069573Z",
     "shell.execute_reply": "2024-05-21T11:17:08.068670Z"
    },
    "papermill": {
     "duration": 0.018824,
     "end_time": "2024-05-21T11:17:08.071474",
     "exception": false,
     "start_time": "2024-05-21T11:17:08.052650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    base_path: Path = Path(\"/kaggle/input/image-matching-challenge-2024\")\n",
    "    feature_dir: Path = Path.cwd() / \".feature_outputs\"\n",
    "        \n",
    "    device: torch.device = K.utils.get_cuda_device_if_available(0)\n",
    "    \n",
    "    pair_matching_args = {\n",
    "        \"model_name\": \"/kaggle/input/dinov2/pytorch/large/1\",\n",
    "        \"similarity_threshold\": 0.3,\n",
    "        \"tolerance\": 500,\n",
    "        \"min_matches\": 110,\n",
    "        \"exhaustive_if_less\": 50,\n",
    "        \"p\": 2.0,\n",
    "    }\n",
    "    \n",
    "    keypoint_detection_args = {\n",
    "        \"num_features\": 6000,\n",
    "        \"resize_to\": 1280,\n",
    "    }\n",
    "    \n",
    "    keypoint_distances_args = {\n",
    "        \"min_matches\": 110,\n",
    "        \"verbose\": False,\n",
    "    }\n",
    "    \n",
    "    colmap_mapper_options = {\n",
    "        \"min_model_size\": 5, # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n",
    "        \"max_num_models\": 3,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d8c785b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:17:08.092711Z",
     "iopub.status.busy": "2024-05-21T11:17:08.092457Z",
     "iopub.status.idle": "2024-05-21T11:17:08.109682Z",
     "shell.execute_reply": "2024-05-21T11:17:08.108939Z"
    },
    "papermill": {
     "duration": 0.030324,
     "end_time": "2024-05-21T11:17:08.111516",
     "exception": false,
     "start_time": "2024-05-21T11:17:08.081192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_from_config(config: Config) -> None:\n",
    "    results = {}\n",
    "    \n",
    "    data_dict = parse_sample_submission(config.base_path)\n",
    "    datasets = list(data_dict.keys())\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        if dataset not in results:\n",
    "            results[dataset] = {}\n",
    "            \n",
    "        for scene in data_dict[dataset]:\n",
    "            images_dir = data_dict[dataset][scene][0].parent\n",
    "            results[dataset][scene] = {}\n",
    "            image_paths = data_dict[dataset][scene]\n",
    "            print (f\"Got {len(image_paths)} images\")\n",
    "            \n",
    "            try:\n",
    "                feature_dir = config.feature_dir / f\"{dataset}_{scene}\"\n",
    "                feature_dir.mkdir(parents=True, exist_ok=True)\n",
    "                database_path = feature_dir / \"colmap.db\"\n",
    "                if database_path.exists():\n",
    "                    database_path.unlink()\n",
    "                \n",
    "                index_pairs = get_image_pairs(\n",
    "                    image_paths,\n",
    "                    **config.pair_matching_args,\n",
    "                    device=config.device,\n",
    "                )\n",
    "                gc.collect()\n",
    "                \n",
    "                detect_keypoints(\n",
    "                    image_paths,\n",
    "                    feature_dir,\n",
    "                    **config.keypoint_detection_args,\n",
    "                    device=device,\n",
    "                )\n",
    "                gc.collect()\n",
    "\n",
    "                keypoint_distances(\n",
    "                    image_paths, \n",
    "                    index_pairs, \n",
    "                    feature_dir,\n",
    "                    **config.keypoint_distances_args,\n",
    "                    device=device,\n",
    "                )\n",
    "                gc.collect()\n",
    "                \n",
    "                sleep(1)\n",
    "                \n",
    "                import_into_colmap(\n",
    "                    images_dir, \n",
    "                    feature_dir, \n",
    "                    database_path,\n",
    "                )\n",
    "                \n",
    "                output_path = feature_dir / \"colmap_rec_aliked\"\n",
    "                output_path.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                pycolmap.match_exhaustive(database_path)\n",
    "                \n",
    "                mapper_options = pycolmap.IncrementalPipelineOptions(**config.colmap_mapper_options)\n",
    "                \n",
    "                maps = pycolmap.incremental_mapping(\n",
    "                    database_path=database_path, \n",
    "                    image_path=images_dir,\n",
    "                    output_path=output_path, \n",
    "                    options=mapper_options,\n",
    "                )\n",
    "                \n",
    "                print(maps)\n",
    "                clear_output(wait=False)\n",
    "                \n",
    "                images_registered  = 0\n",
    "                best_idx = None\n",
    "                \n",
    "                print (\"Looking for the best reconstruction\")\n",
    "            \n",
    "                if isinstance(maps, dict):\n",
    "                    for idx1, rec in maps.items():\n",
    "                        print(idx1, rec.summary())\n",
    "                        try:\n",
    "                            if len(rec.images) > images_registered:\n",
    "                                images_registered = len(rec.images)\n",
    "                                best_idx = idx1\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                \n",
    "                if best_idx is not None:\n",
    "                    for k, im in maps[best_idx].images.items():\n",
    "                        key = config.base_path / \"test\" / scene / \"images\" / im.name\n",
    "                        results[dataset][scene][key] = {}\n",
    "                        results[dataset][scene][key][\"R\"] = deepcopy(im.cam_from_world.rotation.matrix())\n",
    "                        results[dataset][scene][key][\"t\"] = deepcopy(np.array(im.cam_from_world.translation))\n",
    "                        \n",
    "                print(f\"Registered: {dataset} / {scene} -> {len(results[dataset][scene])} images\")\n",
    "                print(f\"Total: {dataset} / {scene} -> {len(data_dict[dataset][scene])} images\")\n",
    "                create_submission(results, data_dict, config.base_path)\n",
    "                gc.collect()\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b630afd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:17:08.132011Z",
     "iopub.status.busy": "2024-05-21T11:17:08.131753Z",
     "iopub.status.idle": "2024-05-21T11:23:39.098865Z",
     "shell.execute_reply": "2024-05-21T11:23:39.097564Z"
    },
    "papermill": {
     "duration": 390.979819,
     "end_time": "2024-05-21T11:23:39.101107",
     "exception": false,
     "start_time": "2024-05-21T11:17:08.121288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for the best reconstruction\n",
      "0 Reconstruction:\n",
      "\tnum_reg_images = 39\n",
      "\tnum_cameras = 39\n",
      "\tnum_points3D = 17244\n",
      "\tnum_observations = 95399\n",
      "\tmean_track_length = 5.5323\n",
      "\tmean_observations_per_image = 2446.13\n",
      "\tmean_reprojection_error = 0.936869\n",
      "Registered: church / church -> 39 images\n",
      "Total: church / church -> 41 images\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00046.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00090.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00092.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00087.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00050.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00068.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00083.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00096.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00069.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00081.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00042.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00018.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00030.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00024.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00032.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00026.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00037.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00008.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00035.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00021.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00010.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00039.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00011.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00013.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00006.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00012.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00029.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00001.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00072.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00066.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00058.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00059.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00111.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00061.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00060.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00074.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00102.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00076.png\n",
      "/kaggle/input/image-matching-challenge-2024/test/church/images/00063.png\n"
     ]
    }
   ],
   "source": [
    "run_from_config(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20340318",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:23:39.122993Z",
     "iopub.status.busy": "2024-05-21T11:23:39.122707Z",
     "iopub.status.idle": "2024-05-21T11:23:40.100378Z",
     "shell.execute_reply": "2024-05-21T11:23:40.099363Z"
    },
    "papermill": {
     "duration": 0.990946,
     "end_time": "2024-05-21T11:23:40.102541",
     "exception": false,
     "start_time": "2024-05-21T11:23:39.111595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf441abd",
   "metadata": {
    "papermill": {
     "duration": 0.010191,
     "end_time": "2024-05-21T11:23:40.123226",
     "exception": false,
     "start_time": "2024-05-21T11:23:40.113035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What to do next?\n",
    "\n",
    "Here are some ways in which you can explore potential improvements:\n",
    "\n",
    "- Using a different image embedding model to obtain the image pairs\n",
    "- Trying other approaches for keypoint extraction, such as SIFT or DISK\n",
    "- Leveraging the training data to train a better models for each dataset"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8143495,
     "sourceId": 71885,
     "sourceType": "competition"
    },
    {
     "datasetId": 3745604,
     "sourceId": 6482802,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4628331,
     "sourceId": 7884725,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4883116,
     "sourceId": 8233572,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 2663,
     "sourceId": 3736,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 3327,
     "sourceId": 4535,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 429.210696,
   "end_time": "2024-05-21T11:23:43.285231",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-21T11:16:34.074535",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
